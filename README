This is a temporary repo for code that measures intersecting prefetches between those listed on traces/prefetchers_KEY.txt

New Update: Some traces are too large, so 1% of the prefetches are randomly (seed: 1337) sampled to represent them. The names of these benchmarks can be found on traces/large_benchmarks.txt

Current KEY:

0: bingo,bop_
1: bingo,scooby_
2: bingo,sisb_
3: bingo,sms_
4: bingo,spp_
5: bingo_
6: bop,scooby_
7: bop,sisb_
8: bop,sms_
9: bop,spp_
10: bop_
11: scooby,sisb_
12: scooby,sms_
13: scooby,spp_
14: scooby_
15: sisb,sms_
16: sisb,spp_
17: sisb_
18: sms,spp_
19: sms_
20: spp_

Reading Results:

"Total Prefetches: X Evaluated Prefetches: Y" 
    - On Sampled Results: X (number of requests read, duplicates included), Y: (number of requests added to the table, duplicates included)
    - Random Numbers chosen on the seed srand(1337) with a .01 probability of allowing a prefetch to be added, but this represents the true fraction.

"Total Unique Prefetches By All": Total # of *Unique* Prefetches by All Prefetchers 
    - If Prefetchers X, Y share Prefetch Request K, then K is only counted once.

"Unique Prefetches By X": Number of Unique Prefetches by *only* X (exclusive)
    - If Prefetchers X, Y share Prefetch Request K, then K is *not* counted in this metric.

"Prefetches Shared by (X, Y)": Number of Prefetches Shared between X, Y (inclusive)
    - If Prefetchers X, Y, Z share Prefetch Request K, then K is counted between (X, Y), (X, Z), *and* (Y,Z).

The raw data might not look like its adds up, but remember that it takes a 21-dimensional principles of inclusion/exclusion calculation to fully make sense of the data structure.


Assumes read permissions to /scratch/cluster/azheng on UT Austin Cluster
Results also viewable on /scratch/cluster/azheng/results

To run, simply call (on a UT lab machine):

python3 file_sweep.py
g++ -o count count.cc
./count

UPDATE: This is a very heavy task. We use the condor job submission to deal with it instead of running the binary straight up like below. Please see traces/large_benchmarks.txt for a list of benchmarks that may cause the lab machine to run out of memory. 

g++ -o count_random count_random.cc
./count_random

Current progress measured on traces/progress_random.txt while running

UPDATE 2: 

to count the footprint instead, use count_footprint.cc
